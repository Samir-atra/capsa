<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" />

    <!-- Generated with Sphinx 6.1.3 and Furo 2022.12.07 -->
        <title>ThemisAI Bias Detection Demo - capsa documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../index.html"><div class="brand">capsa  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../index.html">
  
  
  <span class="sidebar-brand-text">capsa  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/index.html">üëã Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/installation.html">üíæ Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../risk_metrics/index.html">‚≠êÔ∏è Risk Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/basic_usage.html">üé¨ Basic Usage</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../index.html">üë©‚Äçüè´ Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../1_Ensemble-Classification.html">Ensemble Wrapper (Classification)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2_Histogram-Classification.html">Histogram Wrapper (Classification)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_MVE-Classification.html">MVE Wrapper (Classification)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../4_MVE-Regression.html">MVE Wrapper (Regression)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5_High-Dimensional-Depth.html">High Level Introduction to Capsa with a Real World Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../6_OOD-Detection.html">Anomaly Detection (Advanced)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../7_Composability.html">Composability (Advanced)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api_documentation/index.html">‚Äçüíª Metric Wrapper API</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/BaseWrapper.html">BaseWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/RiskTensor.html">RiskTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/MVEWrapper.html">MVEWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/VAEWrapper.html">VAEWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/EnsembleWrapper.html">EnsembleWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/DropoutWrapper.html">DropoutWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/HistogramWrapper.html">HistogramWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_documentation/HistogramVAEWrapper.html">HistogramVAEWrapper</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute/index.html">üßø Contribute</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<section id="ThemisAI-Bias-Detection-Demo">
<h1>ThemisAI Bias Detection Demo<a class="headerlink" href="#ThemisAI-Bias-Detection-Demo" title="Permalink to this heading">#</a></h1>
<section id="Capsa-automatically-wraps-ML-workflows-to-achieve-risk-aware-learning-and-deployment.">
<h2>Capsa automatically wraps ML workflows to achieve risk-aware learning and deployment.<a class="headerlink" href="#Capsa-automatically-wraps-ML-workflows-to-achieve-risk-aware-learning-and-deployment." title="Permalink to this heading">#</a></h2>
<p>In this demo, we‚Äôll be demonstrating how <strong>CAPSA</strong>, the model-agnostic risk awareness framework that we‚Äôve developed at Themis AI, can automatically detect bias in datasets of faces. Here, the dataset we‚Äôve chosen here is the Celeb-A dataset, which containts over 200K celebrity images. We create a face classification task by choosing random samples from ImageNet as negatives.</p>
<p>First, we import the necessary dependencies, set some important hyperparameters, and load the data</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import cv2
import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import time
import h5py
import sys
import glob
import functools
from train_dataloader import TrainingDatasetLoader
from tqdm import tqdm
from helper import plot_k, plot_percentile, plot_frequencies
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>training = False
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>latent_dim = 100
batch_size = 32
num_epochs = 6
</pre></div>
</div>
</div>
</section>
<section id="Load,-prepare,-and-visualize-the-data">
<h2>Load, prepare, and visualize the data<a class="headerlink" href="#Load,-prepare,-and-visualize-the-data" title="Permalink to this heading">#</a></h2>
<p>We normalize all data to be on a 0-1 scale.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data_path = tf.keras.utils.get_file(
    &quot;train_face.h5&quot;, &quot;https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1&quot;
)
dataloader = TrainingDatasetLoader(data_path, batch_size=batch_size)
sample_batch_imgs, sample_batch_labels = dataloader.return_sample_batch()
test_dataloader = TrainingDatasetLoader(data_path, batch_size, training=False)
selected_inds = test_dataloader.pos_train_inds
sorted_inds = np.sort(selected_inds)
test_img = (test_dataloader.images[sorted_inds, :, :, ::-1] / 255.0).astype(np.float32)
test_label = test_dataloader.labels[sorted_inds, ...]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Opening /home/sadhanalolla/.keras/datasets/train_face.h5
Loading data into memory...
Opening /home/sadhanalolla/.keras/datasets/train_face.h5
Loading data into memory...
</pre></div></div>
</div>
<p>Here‚Äôs an example face from the dataset:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.imshow(test_img[4])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f962ee377f0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_8_1.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_8_1.png" />
</div>
</div>
<p>Running some preliminary analyses on the dataset, we can see that it is clearly biased in favor of females:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_frequencies()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_10_0.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_10_0.png" />
</div>
</div>
</section>
<section id="Create-a-baseline-CNN-model-for-classifying-this-data">
<h2>Create a baseline CNN model for classifying this data<a class="headerlink" href="#Create-a-baseline-CNN-model-for-classifying-this-data" title="Permalink to this heading">#</a></h2>
<p>We create a standard CNN to classify the images as faces or negatives</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>&quot;&quot;&quot;Function to define a standard CNN model&quot;&quot;&quot;

def make_standard_classifier(n_outputs=1, n_filters=12):
  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding=&#39;same&#39;, activation=&#39;relu&#39;)
  BatchNormalization = tf.keras.layers.BatchNormalization
  Flatten = tf.keras.layers.Flatten
  Dense = functools.partial(tf.keras.layers.Dense, activation=&#39;relu&#39;)

  model = tf.keras.Sequential([
    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),

    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),

    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Flatten(),
    Dense(512),
    Dense(n_outputs, activation=None),
  ])
  model.build((None, 64, 64, 3))
  return model

standard_classifier = make_standard_classifier()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2022-08-30 10:01:39.488782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:39.517840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:39.518867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:39.520603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-30 10:01:39.525685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:39.526702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:39.527717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:40.120773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:40.121794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:40.122772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-30 10:01:40.123746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22336 MB memory:  -&gt; device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:61:00.0, compute capability: 8.6
</pre></div></div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">call</span></code> method, we can generate predictions on some sample images with our regular Keras model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sample_output = standard_classifier(sample_batch_imgs)
print(sample_output)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2022-08-30 10:01:41.002986: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tf.Tensor(
[[-0.03072214]
 [-0.04305155]
 [-0.03047643]
 [-0.02251552]
 [-0.03632434]
 [-0.04556753]
 [-0.02091157]
 [-0.04374176]
 [-0.04210186]
 [-0.02820173]
 [-0.03191389]
 [-0.01701416]
 [-0.02477791]
 [-0.00503448]
 [-0.02725303]
 [-0.0089234 ]
 [-0.02269083]
 [-0.00886191]
 [-0.03240809]
 [-0.02464975]
 [-0.00787145]
 [-0.01033578]
 [-0.00864111]
 [-0.02661331]
 [-0.02721798]
 [-0.01883158]
 [-0.00218547]
 [-0.02634273]
 [-0.03001968]
 [-0.04419523]
 [-0.00049335]
 [-0.02352858]], shape=(32, 1), dtype=float32)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2022-08-30 10:01:42.778695: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
</pre></div></div>
</div>
<section id="Integrating-Capsa-to-Create-Risk-Aware-AI-Models">
<h3>Integrating Capsa to Create Risk-Aware AI Models<a class="headerlink" href="#Integrating-Capsa-to-Create-Risk-Aware-AI-Models" title="Permalink to this heading">#</a></h3>
<p>Now, we import capsa, our risk-aware framework. We‚Äôll be showing two different ways of ‚Äúwrapping‚Äù our models so that they are risk aware.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from capsa import wrap, VAEWrapper, HistogramCallback
</pre></div>
</div>
</div>
</section>
</section>
<section id="Method-1:-The-wrap(‚Ä¶)-function">
<h2>Method 1: The <em>wrap(‚Ä¶)</em> function<a class="headerlink" href="#Method-1:-The-wrap(‚Ä¶)-function" title="Permalink to this heading">#</a></h2>
<p>Now, we wrap the standard classifier. In this example, we choose the VAE, or variational autoencoder, and analyze the biases in the dataset. The VAE can also be used to quantify epistemic uncertainty, which we will show here as well. With this one line, we‚Äôve now created a risk-aware model!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>wrapped_classifier = wrap(standard_classifier, aleatoric=False)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
wrapping with: [&lt;capsa.epistemic.VAE.VAEWrapper object at 0x7f962cd09b80&gt;]
</pre></div></div>
</div>
</section>
<section id="Method-2:-Directly-calling-metric-wrapper">
<h2>Method 2: Directly calling metric wrapper<a class="headerlink" href="#Method-2:-Directly-calling-metric-wrapper" title="Permalink to this heading">#</a></h2>
<p>An alternate method of wrapping the model is using the Wrapper itself, and this is useful when we have more hyperparameters that we want to specify.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>wrapped_classifier = VAEWrapper(
    standard_classifier,
    latent_dim=latent_dim,
    epistemic=True,
)
</pre></div>
</div>
</div>
</section>
<section id="Capsa-models-are-drop-in-replacements-for-existing-models">
<h2>Capsa models are drop-in replacements for existing models<a class="headerlink" href="#Capsa-models-are-drop-in-replacements-for-existing-models" title="Permalink to this heading">#</a></h2>
<p>We choose hyperparameters, load our training data, and compile the model just as we would a normal Keras model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Training hyperparameters
learning_rate = 1e-5

wrapped_classifier.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate),
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)
</pre></div>
</div>
</div>
<p>Lastly, we fit the model. We can use any of the capabilities of a normal Keras model with the wrapped model, and fit is no exception.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if training:
    history = wrapped_classifier.fit(
        dataloader,
        epochs=6,
        batch_size=batch_size,
        validation_data=test_dataloader,
        callbacks=[HistogramCallback()]
    )
else:
    history = wrapped_classifier.fit(
        dataloader,
        epochs=1,
        batch_size=batch_size,
        validation_data=test_dataloader,
        callbacks=[HistogramCallback()]
    )
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2404/2404 [==============================] - 48s 19ms/step - loss: 0.1000 - binary_accuracy: 0.9626 - val_loss: 0.0413 - val_binary_accuracy: 0.9862
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if training:
    wrapped_classifier.save_weights(&quot;wrapped_model_bias_and_epistemic_shuffled_0&quot;)
else:
    path = &quot;/data/capsa/vae_face/&quot;
    wrapped_classifier.load_weights(path + &quot;wrapped_model_bias_and_epistemic_shuffled_0&quot;)
</pre></div>
</div>
</div>
<p>Like before, we can call the Keras model again on a sample batch; this time, however, we get the output as well as the epistemic uncertainty per sample and the bias per sample</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>prediction, uncertainty, bias = wrapped_classifier(sample_batch_imgs)
print(f&quot;Number of prediction outputs: {len(prediction)}&quot;)
print(f&quot;Number of uncertainty outputs: {len(uncertainty)}&quot;)
print(f&quot;Number of bias outputs: {len(bias)}&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of prediction outputs: 32
Number of uncertainty outputs: 32
Number of bias outputs: 32
</pre></div></div>
</div>
<p>Now that our model is trained, we load the test dataset.</p>
<p>We call the wrapped classifier on the test dataset and sort the dataset in order of increasing bias.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>predictions, uncertainty, bias = wrapped_classifier(test_img, softmax=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>bias = np.squeeze(bias)
sorted_bias_inds = np.argsort(bias)
sorted_biases = np.array(bias[sorted_bias_inds])
sorted_images = np.array(test_img[sorted_bias_inds])
sorted_labels = np.array(test_label[sorted_bias_inds])
sorted_predictions = np.array(predictions)[sorted_bias_inds]
</pre></div>
</div>
</div>
<p>Shown below are the 20 images with the lowest bias (i.e.¬†the least represented in the dataset). We can see that varied facial position and obstructing items such as hats and sunglasses are underrepresented in this dataset</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_k(sorted_images[:20])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_35_0.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_35_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<p>Here we see the top 20 samples that the dataset is biased towards. These are overwhemingly light-skinned females with light hair, and almost all the people in these images are directly facing the camera</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_k(sorted_images[-20:])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_37_0.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_37_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<p>Now, we plot the average face by percentile. The top left image shows the average face of the bottom 5th percentile in terms of bias, and we can see that the average skin tone and hair color is darker than the faces in the higher percentiles.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>num_percentile_intervals = 10
num_samples = len(test_img) // num_percentile_intervals
all_imgs = []
all_bias = []
all_acc = []
for percentile in range(num_percentile_intervals):
    cur_imgs = sorted_images[percentile * num_samples : (percentile + 1) * num_samples]
    cur_bias = sorted_biases[percentile * num_samples : (percentile + 1) * num_samples]
    cur_labels = sorted_labels[percentile * num_samples : (percentile + 1) * num_samples]
    cur_predictions = tf.nn.sigmoid(sorted_predictions[percentile * num_samples : (percentile + 1) * num_samples])
    avged_imgs = tf.reduce_mean(cur_imgs, axis=0)
    all_imgs.append(avged_imgs)
    all_bias.append(tf.reduce_mean(cur_bias))
    all_acc.append((cur_labels == np.rint(cur_predictions)).mean())
</pre></div>
</div>
</div>
<p>Next, we sort the data according to the biases and plot the average accuracy as a function of the percentile. We see here that the items with extremely low bias also have lower accuracy.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.plot(np.arange(num_percentile_intervals) * 10, all_acc)
plt.title(&quot;Percentile vs. average accuracy&quot;)
plt.show()
plt.clf()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_41_0.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_41_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<p>Here‚Äôs the average face according to percentile: as the bias in the dataset increases, the skin tone gets lighter, hair color gets lighter, and the average face becomes much more defined.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_percentile(all_imgs)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_43_1.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_43_1.png" />
</div>
</div>
<p>By including a single line in our normal training workflow, we‚Äôve found significant bias in our dataset!</p>
<p>Now, we estimate the epistemic uncertainty (or model uncertainty). We see here that the items with the highest epistemic uncertainty have occlusions (shadows, masks, glasses) or abnormal lighting. This indicates that the model is the most uncertain about these predictions, even if they have a high prediction value.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>uncertainty = np.squeeze(uncertainty)
sorted_uncertainty_inds = np.argsort(uncertainty)
sorted_uncertainty = np.array(uncertainty[sorted_uncertainty_inds])
sorted_uncertainty_images = np.array(test_img[sorted_uncertainty_inds])
sorted_uncertainty_labels = np.array(test_label[sorted_uncertainty_inds])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_k(sorted_uncertainty_images[-20:])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_46_0.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_46_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div></div>
</div>
<p>Using capsa, we can also estimate the pixel-wise uncertainty of a certain image. For the specific image shown, the mouth and background have little to no uncertainty because they are well-represented in the dataset, while the contours of the mask are highlighted as this is the region the model is the most uncertain about.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_, _, pixel_wise_uncertainty, _ = wrapped_classifier(np.expand_dims(test_img[8741], 0), per_pixel=True)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.imshow(test_img[8741])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f94e42991f0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_49_1.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_49_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.imshow(tf.squeeze(pixel_wise_uncertainty[:, :, :, 2]), cmap=&#39;hot&#39;, interpolation=&#39;nearest&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f951c262550&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../_images/tutorials_notebooks_extra_face_tutorial_50_1.png" src="../../../_images/tutorials_notebooks_extra_face_tutorial_50_1.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Themis AI Inc
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">ThemisAI Bias Detection Demo</a><ul>
<li><a class="reference internal" href="#Capsa-automatically-wraps-ML-workflows-to-achieve-risk-aware-learning-and-deployment.">Capsa automatically wraps ML workflows to achieve risk-aware learning and deployment.</a></li>
<li><a class="reference internal" href="#Load,-prepare,-and-visualize-the-data">Load, prepare, and visualize the data</a></li>
<li><a class="reference internal" href="#Create-a-baseline-CNN-model-for-classifying-this-data">Create a baseline CNN model for classifying this data</a><ul>
<li><a class="reference internal" href="#Integrating-Capsa-to-Create-Risk-Aware-AI-Models">Integrating Capsa to Create Risk-Aware AI Models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Method-1:-The-wrap(‚Ä¶)-function">Method 1: The <em>wrap(‚Ä¶)</em> function</a></li>
<li><a class="reference internal" href="#Method-2:-Directly-calling-metric-wrapper">Method 2: Directly calling metric wrapper</a></li>
<li><a class="reference internal" href="#Capsa-models-are-drop-in-replacements-for-existing-models">Capsa models are drop-in replacements for existing models</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>